# Notes on CNN

The conventional 1 hidden layer NN excelled in making short computation and was very easy to train using backpropagation. In its architecture, each input neuron was strictly connected to every neuron in the hidden layer. Hence by property of combination, there were i * h synapses, i corresponding to input layer length, and h corresponding to hidden layer length.

Since images are numerically represented using their color intensity/RGB indicators in 0-255 range, a small 32 by 32 bit image takes 1024 input neurons. This combined with the fact that hidden layers are often the same size if not larger than the input layer arises the problem of extreme amounts of computation. Even a hidden layer with the same number of elements (1024) would lead to a staggering 1048576 synapses (and each synapses has associated with it one multiplication). This is again, just for the first hidden layer.

Convolutional Neural Networks work around this in the sense that is groups certain sizes of input neurons together, which all correspond to one hidden layer. This leads to new variables. Condider the batch size 'b'. This indicates the number corresponding to how the inputs are grouped. In the example below, that is 3. The stride length indicates the distance between steps (groupings). In the example below that is 1 since the entire batch collectively moves one step to the right. Ofcourse, now the new input batch has 3 different inputs, namely the 3 new inputs in the newly introduced column to the far right. The 1st and 2nd column were in the first batch also. 

ooo.......      .ooo......                                                                                                  ooo.......		  .ooo......                                                                                                      
ooo.......	  	.ooo......                                                                                                      
..........  		..........                                                                                                      
.......... -->	..........                                                                                                       
..........      ..........                                                                                                       
..........      ..........                                                                                                       
..........      ..........                                                                                                       
..........      ..........                                                                                                       
..........      ..........                                                                                                       

Hence by mathematical reasoning we see that the hidden layer, in size is less than the input layer. In the example above we have an input size of 10 x 10, a batch size of 3 and a stride length of 1 so the hidden layer is 8 x 8. And also each hidden layer neuron is only considering computations coming from 9 synapses. (3 x 3 batch size)

*To be further discussed...*


- Naman Pujari
